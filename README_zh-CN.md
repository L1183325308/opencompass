<div align="center">
  <img src="docs/zh_cn/_static/image/FinEval.jpg" width="500px"/>
  <br />
  <br />

[![docs](https://readthedocs.org/projects/opencompass/badge)](https://opencompass.readthedocs.io/zh_CN)
[![license](https://img.shields.io/github/license/InternLM/opencompass.svg)](https://github.com/InternLM/opencompass/blob/main/LICENSE)

<!-- [![PyPI](https://badge.fury.io/py/opencompass.svg)](https://pypi.org/project/opencompass/) -->

[ğŸŒWebsite](https://opencompass.org.cn/) |
[ğŸ“˜Documentation](https://opencompass.readthedocs.io/zh_CN/latest/index.html) |
[ğŸ› ï¸Installation](https://opencompass.readthedocs.io/zh_CN/latest/get_started.html) |
[ğŸ¤”Reporting Issues](https://github.com/InternLM/opencompass/issues/new/choose)

[English](/README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> å’Œ <a href="https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce" target="_blank">å¾®ä¿¡ç¤¾åŒº</a>
</p>

æ¬¢è¿æ¥åˆ°OpenCompassï¼

å°±åƒæŒ‡å—é’ˆåœ¨æˆ‘ä»¬çš„æ—…ç¨‹ä¸­ä¸ºæˆ‘ä»¬å¯¼èˆªä¸€æ ·ï¼Œæˆ‘ä»¬å¸Œæœ›OpenCompassèƒ½å¤Ÿå¸®åŠ©ä½ ç©¿è¶Šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡é‡è¿·é›¾ã€‚OpenCompassæä¾›ä¸°å¯Œçš„ç®—æ³•å’ŒåŠŸèƒ½æ”¯æŒï¼ŒæœŸå¾…OpenCompassèƒ½å¤Ÿå¸®åŠ©ç¤¾åŒºæ›´ä¾¿æ·åœ°å¯¹NLPæ¨¡å‹çš„æ€§èƒ½è¿›è¡Œå…¬å¹³å…¨é¢çš„è¯„ä¼°ã€‚



## ä»‹ç»

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç„¶è€Œå®ƒä»¬åœ¨æ›´å…·æŒ‘æˆ˜æ€§å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„æ•ˆåŠ›ä»ç„¶å¾ˆå°‘è¢«æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†FinEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºLLMsä¸­çš„é‡‘èé¢†åŸŸçŸ¥è¯†è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚

FinEvalæ˜¯ä¸€ä¸ªåŒ…å«**é‡‘èã€ç»æµã€ä¼šè®¡å’Œè¯ä¹¦**ç­‰é¢†åŸŸé«˜è´¨é‡å¤šé¡¹é€‰æ‹©é¢˜çš„é›†åˆã€‚å®ƒåŒ…æ‹¬äº†4,738ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†34ä¸ªä¸åŒçš„å­¦ç§‘ã€‚ä¸ºäº†ç¡®ä¿å¯¹æ¨¡å‹æ€§èƒ½çš„å…¨é¢è¯„ä¼°ï¼ŒFinEvalé‡‡ç”¨äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€ä»…ç­”æ¡ˆå’Œé“¾å¼æ€ç»´æç¤ºç­‰å¤šç§æ–¹æ³•ã€‚åœ¨FinEvalä¸Šè¯„ä¼°æœ€å…ˆè¿›çš„ä¸­æ–‡å’Œè‹±æ–‡LLMsï¼Œç»“æœæ˜¾ç¤ºåªæœ‰GPT-4åœ¨ä¸åŒçš„æç¤ºè®¾ç½®ä¸‹è¾¾åˆ°äº†70%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜LLMsåœ¨é‡‘èé¢†åŸŸçŸ¥è¯†æ–¹é¢å…·æœ‰æ˜¾è‘—çš„å¢é•¿æ½œåŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„é‡‘èçŸ¥è¯†è¯„ä¼°åŸºå‡†ï¼Œåˆ©ç”¨çº¸è´¨å®è·µé¢˜ç›®ï¼Œæ¶µç›–äº†å¹¿æ³›çš„LLMsè¯„ä¼°èŒƒå›´ã€‚


## æ€§èƒ½æ¦œå•

æˆ‘ä»¬åˆ†ä¸ºAnswer-onlyå’ŒCoTå¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œä¸‹é¢æ˜¯æ¨¡å‹çš„zero-shotå’Œfive-shotå‡†ç¡®ç‡ã€‚

### Answer-only

#### Zero-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


#### Five-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


### CoT (chain-of-thought)

#### Zero-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


#### Five-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


## å®‰è£…

ä¸‹é¢å±•ç¤ºäº†å¿«é€Ÿå®‰è£…çš„æ­¥éª¤ï¼Œè¯¦ç»†è¯·å‚è€ƒ[å®‰è£…æŒ‡å—](https://opencompass.readthedocs.io/zh_cn/latest/get_started.html)ã€‚


 ```python
    conda create --name fineval_venv python=3.8
    conda activate fineval_venv
 ```

```python
    git clone https://github.com/caiweige/FinEval
    cd FinEval
    pip install -r requirements.txt
    
    requirements.txt æ–‡ä»¶å¦‚ä¸‹:
    pandas
    torch
    tqdm
    peft 
    sentencepiece
```

## æ”¯æŒæ–°æ•°æ®é›†

å¦‚æœéœ€è¦æ–°åŠ å…¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼Œè¯·å‚è€ƒ[æ”¯æŒæ–°æ•°æ®é›†](/docs/zh_cn/advanced_guide/new_dataset.md)

## æ¨¡å‹æ”¯æŒ

## è¯„æµ‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](https://opencompass.readthedocs.io/zh_CN/latest/get_started.html#id2)äº†è§£å¦‚ä½•è¿è¡Œä¸€ä¸ªè¯„æµ‹ä»»åŠ¡ã€‚

## å¼•ç”¨

```bibtex
@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/InternLM/OpenCompass}},
    year={2023}
}
```
