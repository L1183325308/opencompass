Welcome to FinEval! ! !
==============================================

FIN-EVAL is the first comprehensive Chinese evaluation suite designed to assess the advanced knowledge and reasoning abilities of underlying models in a Chinese context. FIN-EVAL includes multiple-choice questions at four difficulty levels: middle school, high school, college, and major. The questions span 52 different disciplines, ranging from the humanities to science and engineering. FIN-EVAL comes with FIN-EVAL HARD, which is a very challenging subset of FIN-EVAL that requires advanced reasoning capabilities to solve. We comprehensively evaluate state-of-the-art LLMs on FIN-EVAL, including models in English and Chinese. The results show that only GPT-4 can achieve an average accuracy above 60%, which shows that there is still much room for improvement in current LLMs.

You can see an example of our dataset in Prompt_ , or check out our paper (put a link?) for more details.


.. _GetStarted:
.. toctree::
   :maxdepth: 1
   :caption: Get Started

   get_started/install.md
   get_started/dataset_pre.md
   get_started/quick_start.md

.. _UserGuides:
.. toctree::
   :maxdepth: 1
   :caption: User Guides

   user_guide/how_to_run.md
   user_guide/config.md
   user_guide/api_model.md
   user_guide/custom_model.md
   user_guide/prompt_viewer.md

.. _Prompt:
.. toctree::
   :maxdepth: 1
   :caption: Prompt

   prompt/overview.md
   prompt/zero_shot.md
   prompt/few_shot.md
   prompt/cot.md

.. _AdvancedGuides:
.. toctree::
   :maxdepth: 1
   :caption: Advanced Guides

   advanced_guides/new_dataset.md
   advanced_guides/new_model.md


.. _Other Notes:
.. toctree::
   :maxdepth: 1
   :caption: Other Notes

   other/how_to_submit.md
   other/Contact_Us.md

Indexes & Tables
==================

* :ref:`genindex`
* :ref:`search`
